# Overview / Motivation

Machine learning has evolved rapidly over the recent years. In the field of natural language processing, large language models such as GPT-3 and PaLM has paved the way towards universal language models. These models can be applied to any task in NLP through finetuning. This meant that there is no longer a need to train a new model from scratch for every single task.

Understanding humour has always been a challenge in the field of Machine learning. In this project, I created a joke generator using GPT-2 and finetuning with joke data scrapped from Reddit. Whilst the model is able to generate jokes that is grammatically correct in language, most of the jokes do not make a lot of sense and are insensitive.

# To use

Run GPT-2 Joke Generator.ipynb, preferably on GPU. 
