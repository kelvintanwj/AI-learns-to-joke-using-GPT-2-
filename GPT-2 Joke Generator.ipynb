{"cells":[{"cell_type":"markdown","metadata":{"id":"srMujzO8uTQj"},"source":["\n","# Import libraries and model\n","\n","Below are scripts for importing libraries for training a joke generator using the GPT-2 Model. Before running the code, do activate the GPU settings through **Runtime/Change Runtime type** in the menu above."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29401,"status":"ok","timestamp":1656426413194,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"IK1D011-h39O","outputId":"3740c8dd-2db1-4a42-9efc-11602bfcf1d1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/huggingface/transformers.git\n","  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-kejuljqp\n","  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-kejuljqp\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2.23.0)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 26.2 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (2022.6.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (3.7.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.64.0)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[K     |████████████████████████████████| 6.6 MB 48.0 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n","\u001b[K     |████████████████████████████████| 101 kB 14.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.21.0.dev0) (4.11.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.0.dev0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.21.0.dev0) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.21.0.dev0) (3.8.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.21.0.dev0) (1.24.3)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.21.0.dev0-py3-none-any.whl size=4497252 sha256=26b99dfca2cd63ce183f21356c4271d81520d2c1975c6f3284e7fe4cec7c24e5\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-4s6d8hij/wheels/90/a5/44/6bcd83827c8a60628c5ad602f429cd5076bcce5f2a90054947\n","Successfully built transformers\n","Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.21.0.dev0\n"]}],"source":["# install transformers from https://huggingface.co/\n","!pip install git+https://github.com/huggingface/transformers.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vv8Qe5ySiOaD"},"outputs":[],"source":["import logging\n","logging.getLogger().setLevel(logging.CRITICAL)\n","\n","import torch\n","import numpy as np\n","\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":145,"referenced_widgets":["a079bdcb29d1418abe490fdfd07f61af","acd36be2479a431eb6a71601cb959313","fcd5942e57d240a48e7dcfd53bd8e33c","44099af126be4ed5bb1417326ca098eb"]},"executionInfo":{"elapsed":43546,"status":"ok","timestamp":1656426500215,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"2atIVTOMiYEu","outputId":"293136c0-43cc-4d3b-b399-f076790f18e0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a079bdcb29d1418abe490fdfd07f61af","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"acd36be2479a431eb6a71601cb959313","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fcd5942e57d240a48e7dcfd53bd8e33c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44099af126be4ed5bb1417326ca098eb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n","model = GPT2LMHeadModel.from_pretrained('gpt2-medium')\n","model = model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"ueCqtt9zvQGK"},"source":["# Let's test out the GPT-2 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Se4ccPs8iapw"},"outputs":[],"source":["# Function to first select topN tokens from the probability list and then based on the selected N word distribution\n","# get random token ID\n","def choose_from_top(probs, n=5):\n","    ind = np.argpartition(probs, -n)[-n:]\n","    top_prob = probs[ind]\n","    top_prob = top_prob / np.sum(top_prob) # Normalize\n","    choice = np.random.choice(n, 1, p = top_prob)\n","    token_id = ind[choice][0]\n","    return int(token_id)\n","\n","# Function to generate text from default GPT-2 model\n","def generate_some_text(input_str, text_len = 50):\n","\n","    cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n","\n","    model.eval()\n","    with torch.no_grad():\n","\n","        for i in range(text_len):\n","            outputs = model(cur_ids, labels=cur_ids)\n","            loss, logits = outputs[:2]\n","            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(only one) batch and the last predicted embedding\n","            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=10) #Randomly(from the given probability distribution) choose the next word from the top n words\n","            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word\n","\n","        output_list = list(cur_ids.squeeze().to('cpu').numpy())\n","        output_text = tokenizer.decode(output_list)\n","        print(output_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4686,"status":"ok","timestamp":1656426548220,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"t50lViaeiraM","outputId":"89793b1a-6e14-43c9-bcc9-b376bd7da146"},"outputs":[{"name":"stdout","output_type":"stream","text":["Three Economists went into a bar. One of them said, \"What do you think about the economy?\" and one of the guys was like, \"It's great.\" I was like, \"Well, that's what we thought before, huh?\" And he was like, \"Well\n"]}],"source":["# test 1: without training on joke dataset, it outputs story without humour\n","generate_some_text('''Three Economists went into a bar.''')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5578,"status":"ok","timestamp":1656155274808,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"a26FZtnPiv3F","outputId":"8512f391-38fe-4e5a-e8ab-cad3e2479f4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Three Economists went into a bar. They started asking each other about what their economic theories were. They started to discuss their theory and then they went to the bar.\n","\n","The economist that was most interested was the bartender who started to ask about his theory.\n","\n","\"What do\n"]}],"source":["# test 2\n","generate_some_text('''Three Economists went into a bar.''')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":588,"status":"ok","timestamp":1656155346771,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"63LtSSSFi-Jv","outputId":"d04dd2d4-4712-4128-82c5-3c72fe736b5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["This learning and development session is going to be a great opportunity for you to get familiar and excited about how we build this system. You will meet other members of the design team, including the technical team, as well as industry experts like the CEO and CTO of the company that is\n"]}],"source":["# test 3\n","generate_some_text('''This learning and development session is going''')"]},{"cell_type":"markdown","metadata":{"id":"tCPvcgHQj1Em"},"source":["Training on Joke dataset scrapped from Reddit"]},{"cell_type":"markdown","metadata":{"id":"UPD6Etk9uG_h"},"source":["# Training (Fine-tuning) on joke dataset scrapped from Reddit"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15848,"status":"ok","timestamp":1656426585465,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"2FZvMPxFjQzF","outputId":"e1271e19-caa1-4549-b8e2-ce7f0ec8a810"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Joke generator/data\n"]}],"source":["# Mount into drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# change directory into data\n","%cd '/content/drive/MyDrive/Joke generator/data/'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DEBZsK1MkMOL"},"outputs":[],"source":["from torch.utils.data import Dataset\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","import csv\n","import json\n","\n","class JokesDataset(Dataset):\n","    def __init__(self, verbose = False):\n","        super().__init__()\n","        # !! change this path to your list of joke to customise training\n","        # short_jokes_path = './shortjokes.csv'\n","        # short_jokes_path = './reddit_jokes.csv'\n","        short_jokes_path = './reddit-cleanjokes.csv'\n","        self.joke_list = []\n","        self.end_of_text_token = \"<|endoftext|>\"\n","        self.verbose = verbose\n","        \n","        with open(short_jokes_path) as csv_file:\n","            csv_reader = csv.reader(csv_file, delimiter=',')\n","            \n","            x = 0\n","            for row in csv_reader:\n","                # print(row)\n","                joke_str = f\"JOKE:{row[1]}{self.end_of_text_token}\"\n","                self.joke_list.append(joke_str)\n","        \n","    def __len__(self):\n","        return len(self.joke_list)\n","\n","    def __getitem__(self, item):\n","        return self.joke_list[item]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":334,"status":"ok","timestamp":1656428880526,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"TqRQV7FHk9sn","outputId":"a46f1de1-9558-4b32-b270-6d783c6ef616"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n"]}],"source":["from transformers import AdamW, get_linear_schedule_with_warmup\n","from torchsummary import summary\n","\n","if torch.cuda.is_available():\n","  device = 'cuda'\n","else:\n","  device = 'cpu'  \n","\n","dataset = JokesDataset()\n","joke_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n","\n","# model params\n","BATCH_SIZE = 16\n","EPOCHS = 5\n","LEARNING_RATE = 3e-5\n","WARMUP_STEPS = 5000\n","MAX_SEQ_LEN = 400\n","\n","# initialising model instance\n","model = model.to(device)\n","model.train()\n","optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps = -1)\n","proc_seq_count = 0\n","sum_loss = 0.0\n","batch_count = 0\n","\n","tmp_jokes_tens = None\n","models_folder = \"trained_models\"\n","if not os.path.exists(models_folder):\n","    os.mkdir(models_folder)\n","\n","# for debugging, set dataset.verbose = True when initialising\n","if dataset.verbose:\n","  print(f'Training on: {torch.cuda.get_device_name(0)}\\n')\n","  print(f'Preview model summary: {print(model)}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206167,"status":"ok","timestamp":1656429123840,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"0EB3jq-klC4m","outputId":"2b39d3e1-f4ab-49e0-ab45-ca7112f414af"},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------------------------------\n","Epoch 1\n","-----------------------------------\n","-----------------------------------\n","Epoch 2\n","-----------------------------------\n","-----------------------------------\n","Epoch 3\n","-----------------------------------\n","-----------------------------------\n","Epoch 4\n","-----------------------------------\n","-----------------------------------\n","Epoch 5\n","-----------------------------------\n"]}],"source":["for epoch in range(EPOCHS):\n","    print(\"-----------------------------------\")\n","    print(\"Epoch %d\" % (epoch+1))\n","    print(\"-----------------------------------\")\n","    \n","    for idx,joke in enumerate(joke_loader):\n","        \n","        #################### \"Fit as many joke sequences into MAX_SEQ_LEN sequence as possible\" logic start ####\n","        joke_tens = torch.tensor(tokenizer.encode(joke[0])).unsqueeze(0).to(device)\n","        #Skip sample from dataset if it is longer than MAX_SEQ_LEN\n","        if joke_tens.size()[1] > MAX_SEQ_LEN:\n","            continue\n","        \n","        #The first joke sequence in the sequence\n","        if not torch.is_tensor(tmp_jokes_tens):\n","            tmp_jokes_tens = joke_tens\n","            continue\n","        else:\n","            #The next joke does not fit in so we process the sequence and leave the last joke \n","            #as the start for next sequence \n","            if tmp_jokes_tens.size()[1] + joke_tens.size()[1] > MAX_SEQ_LEN:\n","                work_jokes_tens = tmp_jokes_tens\n","                tmp_jokes_tens = joke_tens\n","            else:\n","                #Add the joke to sequence, continue and try to add more\n","                tmp_jokes_tens = torch.cat([tmp_jokes_tens, joke_tens[:,1:]], dim=1)\n","                continue\n","        ################## Sequence ready, process it trough the model ##################\n","            \n","        outputs = model(work_jokes_tens, labels=work_jokes_tens)\n","        loss, logits = outputs[:2]                        \n","        loss.backward()\n","        sum_loss = sum_loss + loss.detach().data\n","                       \n","        proc_seq_count = proc_seq_count + 1\n","        if proc_seq_count == BATCH_SIZE:\n","            proc_seq_count = 0    \n","            batch_count += 1\n","            optimizer.step()\n","            scheduler.step() \n","            optimizer.zero_grad()\n","            model.zero_grad()\n","\n","        if batch_count == 100:\n","            print(f\"sum loss {sum_loss}\")\n","            batch_count = 0\n","            sum_loss = 0.0\n","    \n","    # Store the model after each epoch to compare the performance of them\n","    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_reddit_clean_{epoch}.pt\"))\n","\n","\n","models_folder = \"trained_models\""]},{"cell_type":"markdown","metadata":{"id":"cKBQ1Kvux8JD"},"source":["# Generating jokes with/without start words"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wwQ1BjrzlGku"},"outputs":[],"source":["models_folder = \"trained_models\"\n","output_folder = \"output\"\n","num_jokes = 100\n","# change the model path if you trained a new model\n","model_path = os.path.join(models_folder, f\"gpt2_medium_reddit_clean_{0}.pt\")\n","model.load_state_dict(torch.load(model_path))\n","jokes_output_file_path = os.path.join(output_folder, f'bar_generated_clean_{6}.jokes')\n","\n","# determines how the joke should start\n","start_words = \"JOKE: An infinite number of mathematicians walk into a bar.\"\n","\n","model.eval()\n","if os.path.exists(jokes_output_file_path):\n","    os.remove(jokes_output_file_path)\n","    \n","joke_num = 0\n","with torch.no_grad():\n","    for joke_idx in range(num_jokes):\n","        joke_finished = False\n","        cur_ids = torch.tensor(tokenizer.encode(start_words)).unsqueeze(0).to(device)\n","        for i in range(100):\n","            outputs = model(cur_ids, labels=cur_ids)\n","            loss, logits = outputs[:2]\n","            softmax_logits = torch.softmax(logits[0,-1], dim=0) #Take the first(from only one in this case) batch and the last predicted embedding\n","            if i < 3:\n","                n = 20\n","            else:\n","                n = 3\n","            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n) #Randomly(from the topN probability distribution) select the next word\n","            cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim = 1) # Add the last word to the running sequence\n","\n","            if next_token_id in tokenizer.encode('<|endoftext|>'):\n","                joke_finished = True\n","                break\n","        \n","        if joke_finished:\n","            joke_num = joke_num + 1\n","            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n","            output_text = tokenizer.decode(output_list)\n","\n","            with open(jokes_output_file_path, 'a') as f:\n","                f.write(f\"{output_text} \\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":974,"status":"ok","timestamp":1656168843909,"user":{"displayName":"kelvin tan","userId":"00308025688969665934"},"user_tz":-480},"id":"C96ue5I7Wv1J","outputId":"22114d0d-1110-4aec-f8f0-42aea80e4b94"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"JOKE:What's the difference between a gay and a racist? A black man can't get a job at a bank<|endoftext|>\""]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Joke generated are really insensitive ... \n","output_text"]},{"cell_type":"markdown","metadata":{"id":"SH_xEt241NUM"},"source":["# Credits\n","\n","**Data source:**\n","1. https://github.com/amoudgl/short-jokes-dataset\n","2. https://www.kaggle.com/datasets/abhinavmoudgil95/short-jokes\n","\n","**Code adapted from:**\n","1. https://www.kaggle.com/code/leekeonshin/gru-jokes\n","2. https://towardsdatascience.com/teaching-gpt-2-a-sense-of-humor-fine-tuning-large-transformer-models-on-a-single-gpu-in-pytorch-59e8cec40912"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["srMujzO8uTQj","ueCqtt9zvQGK","UPD6Etk9uG_h","cKBQ1Kvux8JD"],"name":"GPT-2 Joke Generator.ipynb","toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyPD0qyVFT5ifMBEm4mpEUgA"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}